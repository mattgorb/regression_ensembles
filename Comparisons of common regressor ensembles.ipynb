{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comparison of Common Regression Ensembles\n",
    "\n",
    "On the following page I train and compare common types of neural network regression ensembles: Mixtures of Experts, Sparsely Gated Mixtures of Experts, and Negative Correlation Learning.  Additionally, I test altering the output activation on Mixtures of Experts models and test the effects of pretraining expert models.   \n",
    "\n",
    "\n",
    "\n",
    "| Model | # Base Models| Sparse Gates (if applicable)   | MSE|\n",
    "|------|------|------|------|\n",
    "|  Single Neural Network| 1|n/a|0.03624|\n",
    "|  Mixtures of Experts  | 4|n/a|0.017827|\n",
    "|   Negative Correlation Learning, lambda=0.5 | 4|n/a|0.0354|\n",
    "|   Negative Correlation Learning, lambda=0.1 | 4|n/a|0.0355601|\n",
    "|   Sparse MoE  | 4|1|0.02036|\n",
    "|   Sparse MoE  | 4|2|0.023|\n",
    "|Sparse MoE    | 8|2|0.02219|\n",
    "|Sparse MoE    | 8|4|0.017976|\n",
    "|  Mixture of Experts, pretrained| 4|n/a|0.01444|\n",
    "|  Mixture of Experts, pretrained with elu gating activation | 4|n/a|0.0010685|\n",
    "|  Sparse MoE , pretrained| 8|2|0.00907|\n",
    "|  Sparse MoE , pretrained| 8|4|0.00049734|\n",
    "|  Sparse MoE , pretrained with elu gating activation| 8|2|0.004674|\n",
    "|  Sparse MoE , pretrained with elu gating activation| 8|4|0.001323|\n",
    "\n",
    "**All neural networks had one hidden node.  Mixture of Experts gating networks had two hidden nodes. Training was for 100 epochs with SGD optimization with a learning rate of 1.   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "-  Pretraining expert networks with distinct subspaces performs better in every case. \n",
    "-  Using alternative gating activation with pretrained experts networks shows promising results. \n",
    "-  Sparse gating shows superior results when expert networks are pretrained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Data is based off the following equation: \n",
    "<img src=\"equation.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "where x = [x1,... x5] is an input vetor whose components lie between zero and one. The value of f (x)\n",
    "lies between 1 and +1. The data onsisted of input/output patterns with the input vetors sampled\n",
    "uniformly at random from the interval (0,1). Training\n",
    "pattern set size was 500, testing set size was 10,000.  \n",
    "\n",
    "\n",
    "### Negative Correlation Learning\n",
    "Equations attained from http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=7E1378BAA5112F9D4775D09B977E80C0?doi=10.1.1.11.1126&rep=rep1&type=pdf\n",
    "<br>To diversify the ensemble and encourage negatively correlated submodels in an ensemble, the following loss function was implemented:\n",
    "<img src=\"ncl.png\" alt=\"Drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "\n",
    "### Mixtures of Experts\n",
    "Equations attained from https://www.cs.toronto.edu/~hinton/absps/Outrageously.pdf\n",
    "#### Loss\n",
    "When end-to-end training mixtures of experts, diversity is encouraged using an additional term in the loss function, the coefficient of variation of the \"importance\".  Importance is equal to the \"the batchwise sum of the gate values for that expert.\" The loss added is equal to the square of the coefficient of variation of the set of importance values, multiplied by a hand-tuned scaling factor importance.\"\n",
    "<img src=\"importance.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "#### Sparse Gating\n",
    "Before taking the softmax of the gating network, we keep only the top-k values.  We set the rest of the values to -infinity because the softmax of -infinity is 0.  \n",
    "<img src=\"sparsity.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "Keras/TensorFlow code for this is below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main methods and parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.layers import Dense,Input,Lambda,Activation\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import sys\n",
    "import os\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, History\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#!{sys.executable} -m pip install sklearn\n",
    "#!{sys.executable} -m pip install keras\n",
    "\n",
    "def generate_input():\n",
    "    return [random.uniform(0, 1),random.uniform(0, 1),random.uniform(0, 1),random.uniform(0, 1),random.uniform(0, 1)]\n",
    "\n",
    "def generate_output(inputs):\n",
    "    x= (1/13)*(10*math.sin(math.pi*inputs[0]*inputs[1])+20*(inputs[2]-.5)**2+10*inputs[3]+5*inputs[4])-1\n",
    "    return x\n",
    "\n",
    "def generate_data(num_to_generate):\n",
    "    inputs=[]\n",
    "    outputs=[]\n",
    "    for a in range(num_to_generate):\n",
    "        x=generate_input()\n",
    "        y=generate_output(x)\n",
    "        inputs.append(x)\n",
    "        outputs.append(y)\n",
    "    return np.array(inputs), np.array(outputs)\n",
    "\n",
    "def getData(test=False):\n",
    "    if(test):\n",
    "        #test data\n",
    "        if(os.path.isfile('test_in.npy') and os.path.isfile('test_out.npy')):\n",
    "            inputs=np.load('test_in.npy')\n",
    "            outputs=np.load('test_out.npy')\n",
    "        else:\n",
    "            inputs,outputs=generate_data(10000)\n",
    "            np.save('test_in.npy', inputs)\n",
    "            np.save('test_out.npy',outputs)\n",
    "    else:\n",
    "        #train data\n",
    "        if(os.path.isfile('train_in.npy') and os.path.isfile('train_out.npy')):\n",
    "            inputs=np.load('train_in.npy')\n",
    "            outputs=np.load('train_out.npy')\n",
    "        else:\n",
    "            inputs,outputs=generate_data(500)\n",
    "            np.save('train_in.npy', inputs)\n",
    "            np.save('train_out.npy',outputs)\n",
    "    return inputs, outputs\n",
    "    \n",
    "\n",
    "\n",
    "def nn(num,H):\n",
    "    nn=Dense(H,kernel_initializer='random_uniform', name='base1_'+str(num))(nn_inputs)\n",
    "    nn=Dense(1,name='base2_'+str(num),kernel_initializer='random_uniform')(nn)\n",
    "    return Model(inputs=nn_inputs, outputs=nn)\n",
    "\n",
    "\n",
    "#Tried different weight initializations\n",
    "#weight_init=keras.initializers.RandomUniform(minval = 0, maxval = 0.05)\n",
    "#weight_init=keras.initializers.Zeros()\n",
    "#weight_init=keras.initializers.RandomNormal(mean=0.1, stddev=0.05, seed=None)\n",
    "#,kernel_initializer=weight_init\n",
    "def moe_(H, num_models):\n",
    "    nn=Dense(H, name='hiddenlayer1')(nn_inputs)\n",
    "    #nn=Activation('sigmoid', name='hiddenactivation')(nn)\n",
    "    nn=Dense(num_models,name='gate')(nn)\n",
    "    return Model(inputs=nn_inputs, outputs=nn)    \n",
    "\n",
    "\n",
    "\n",
    "def ensemble_average(branches):\n",
    "    forLambda=[]\n",
    "    forLambda.extend(branches)\n",
    "    add= Lambda(lambda x:K.tf.transpose(sum(K.tf.transpose(forLambda[i]) for i in range(0,len(forLambda)))/len(forLambda)), name='final')(forLambda)\n",
    "    return add\n",
    "\n",
    "#MOE\n",
    "def gating_multiplier(gate,branches):\n",
    "    forLambda=[gate]\n",
    "    forLambda.extend(branches)\n",
    "    add= Lambda(lambda x:K.tf.transpose(\n",
    "        sum(K.tf.transpose(forLambda[i]) * \n",
    "            forLambda[0][:, i-1] for i in range(1,len(forLambda))\n",
    "           )\n",
    "    ))(forLambda)\n",
    "    return add\n",
    "\n",
    "\n",
    "\n",
    "def slices_to_dims(slice_indices):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    slice_indices: An [N, k] Tensor mapping to column indices.\n",
    "  Returns:\n",
    "    An index Tensor with shape [N * k, 2], corresponding to indices suitable for\n",
    "    passing to SparseTensor.\n",
    "  \"\"\"\n",
    "  slice_indices = tf.cast(slice_indices, tf.int64)\n",
    "  num_rows = tf.shape(slice_indices, out_type=tf.int64)[0]\n",
    "  row_range = tf.range(num_rows)\n",
    "  item_numbers = slice_indices * num_rows + tf.expand_dims(row_range, axis=1)\n",
    "  item_numbers_flat = tf.reshape(item_numbers, [-1])\n",
    "  return tf.stack([item_numbers_flat % num_rows, \n",
    "                   item_numbers_flat // num_rows], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot(model):\n",
    "    figure = plt.figure(figsize=(18, 16))\n",
    "    X = np.arange(0,len(outputs))\n",
    "    tick_plot = figure.add_subplot(1, 1, 1)\n",
    "    tick_plot.plot(X,outputs,  color='green', linestyle='-', marker='*', label='Actual')\n",
    "    tick_plot.plot(X, model.predict(inputs),  color='orange',linestyle='-', marker='*', label='Predictions')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.legend(loc='upper left')\n",
    "    error=model.evaluate(inputs,outputs)\n",
    "    plt.title('Total MSE: '+str(error))\n",
    "    print(\"Ensemble MSE's:\")\n",
    "    print(errors)\n",
    "    plt.show()\n",
    "    \n",
    "def sub_model_errors(model):\n",
    "    errors=[]\n",
    "    test_x, test_y=getData(test=True)\n",
    "    #print(outputs[:1])\n",
    "    for i in range(2,num_models+2):\n",
    "        model1=Model(inputs=nn_inputs, outputs=model.layers[-i].output)\n",
    "        model1.compile(loss='mse', optimizer=SGD(lr=0.1))\n",
    "        errors.append(model1.evaluate(test_x, test_y))\n",
    "    return errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cv_squared(x):\n",
    "  \"\"\"The squared coefficient of variation of a sample.\n",
    "  Useful as a loss to encourage a positive distribution to be more uniform.\n",
    "  Epsilons added for numerical stability.\n",
    "  Returns 0 for an empty Tensor.\n",
    "  Args:\n",
    "    x: a `Tensor`.\n",
    "  Returns:\n",
    "    a `Scalar`.\n",
    "  \"\"\"\n",
    "  epsilon = 1e-10\n",
    "  float_size = tf.to_float(tf.size(x)) + epsilon\n",
    "  mean = tf.reduce_sum(x) / float_size\n",
    "  variance = tf.reduce_sum(tf.squared_difference(x, mean)) / float_size\n",
    "  return variance / (tf.square(mean) + epsilon)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sparseGating(inputs_,gates=2):\n",
    "    indi=tf.cast(tf.math.top_k(inputs_,gates, sorted=False).indices,dtype=tf.int64)\n",
    "    v=tf.math.top_k(inputs_,gates, sorted=False).values\n",
    "\n",
    "    sparse_indices = slices_to_dims(indi)\n",
    "    sparse = tf.SparseTensor( indices=sparse_indices, values=tf.reshape(v, [-1]),\n",
    "                                               dense_shape=tf.cast(tf.shape(inputs_),dtype=tf.int64))\n",
    "    c=tf.zeros_like(inputs_)\n",
    "    d=tf.sparse_add(c, sparse)\n",
    "    z =tf.ones_like(inputs_)*-np.inf\n",
    "    mask = tf.less_equal(d,  tf.zeros_like(d))\n",
    "    new_tensor = tf.multiply(z, tf.cast(mask, dtype=tf.float32))\n",
    "\n",
    "    g=tf.where(tf.is_nan(new_tensor), tf.zeros_like(new_tensor), new_tensor)\n",
    "    g=tf.sparse_add(g,sparse)\n",
    "    b=Lambda(lambda a:g)(inputs_)\n",
    "    return b\n",
    "\n",
    "\n",
    "\n",
    "def preTrain(weight_file):\n",
    "    maxVal=np.amax(train_y)\n",
    "    minVal=np.amin(train_y)\n",
    "    diff=maxVal-minVal\n",
    "    for i in range(num_models):\n",
    "        test_val_min=diff/num_models*i+minVal\n",
    "        test_val_max=diff/num_models*(i+1)+minVal\n",
    "        y=[j for j in range(len(train_y)) if train_y[j]>test_val_min and train_y[j]<test_val_max]\n",
    "        x=train_x[y]\n",
    "        y=train_y[y]\n",
    "\n",
    "        models[i].compile(loss='mse', optimizer=SGD(lr=0.1))\n",
    "        file=str(weight_file)+'_'+str(i)+'.h5'\n",
    "        checkpointer=ModelCheckpoint(file, monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "        models[i].fit(x,y,epochs=100, verbose=1,batch_size=1,callbacks=[checkpointer])\n",
    "\n",
    "def load_weights(model,weight_file):\n",
    "    for a in range(num_models):\n",
    "        m=models[a]\n",
    "        file=str(weight_file)+'_'+str(a)+'.h5'\n",
    "        m.load_weights(file,by_name=True)\n",
    "        for b in m.layers:\n",
    "            for l in model.layers:\n",
    "                if(l.name==b.name):\n",
    "                    l.set_weights(b.get_weights())\n",
    "                    print(\"Loaded: \"+l.name)\n",
    "    for l in model.layers:\n",
    "        if('base' in l.name):\n",
    "            l.trainable=True\n",
    "        else:\n",
    "            print(l.name)\n",
    "\n",
    "def moeLoss(yTrue,yPred):\n",
    "    loss_calc=0\n",
    "    j=0\n",
    "    importance=[]\n",
    "    for i in range(4):\n",
    "        importance.append(tf.reduce_sum(model.get_layer('gate').output[:,i]))\n",
    "    for i in reversed(range(2,num_models+2)):\n",
    "        loss_calc+=(model.get_layer('act').output[:,j]*tf.math.exp(-1/2*(yTrue-model.layers[-i].output)**2))\n",
    "        j+=1\n",
    "    loss_calc=-tf.math.log(loss_calc)\n",
    "    return (loss_calc+.1*cv_squared(importance))\n",
    "\n",
    "\n",
    "def negativeCorrelation(yTrue,yPred):\n",
    "    #return K.mean(K.square(yPred - yTrue), axis=-1)\n",
    "    loss_calc=0\n",
    "    for i in range(2,num_models+2):\n",
    "        others=0\n",
    "        for j in range (2,num_models+2):\n",
    "            if(j==i):\n",
    "                continue\n",
    "            else:\n",
    "                others+=(model.layers[-j].output-model.layers[-1].output)\n",
    "        loss_calc+=1/2*(model.layers[-i].output-yTrue)**2+(lambda_*(model.layers[-i].output-model.layers[-1].output)*(others))\n",
    "    \n",
    "    return loss_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_run='neg_cor'\n",
    "if(model_to_run=='single_nn'):\n",
    "    #Best run on test set MSE: 0.03624\n",
    "    model=nn(\"x\", H)\n",
    "    model.compile(loss='mse', optimizer=SGD(lr=0.1))\n",
    "    model_single_nn=model\n",
    "    checkpointer=ModelCheckpoint('single.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "\n",
    "        \n",
    "\n",
    "elif(model_to_run=='moe'):\n",
    "    #Best run on test set MSE: 0.017827\n",
    "    num_models=4\n",
    "    models=[nn(i, H) for i in range(num_models)]\n",
    "\n",
    "    moe=moe_(expert_nodes, num_models)\n",
    "    layer=Activation(\"softmax\",name='act')(moe.output)\n",
    "    gate=gating_multiplier(layer,[m.layers[-1].output for m in models])\n",
    "    model=Model(inputs=nn_inputs, outputs=gate)\n",
    "    model.compile(loss=moeLoss, optimizer=SGD(lr=0.1))\n",
    "    checkpointer=ModelCheckpoint('moe.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "\n",
    "    \n",
    "elif(model_to_run=='neg_cor'):\n",
    "\n",
    "    num_models=4\n",
    "    lambda_=0.1\n",
    "    models=[nn(i, H) for i in range(num_models)]\n",
    "    model_out=ensemble_average([models[0].output,models[1].output,models[2].output,models[3].output])\n",
    "    model=Model(inputs=nn_inputs, outputs=model_out)\n",
    "    model.compile(loss=negativeCorrelation, optimizer=SGD(lr=0.1))\n",
    "    checkpointer=ModelCheckpoint('averages.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "\n",
    "\n",
    "elif(model_to_run=='sparse'):\n",
    "    #Best run on test set MSE models=4, gates=1: 0.02036\n",
    "    #Best run on test set MSE models=4, gates=2: 0.023095\n",
    "\n",
    "    #Best run on test set MSE models=8, gates=2: 0.02219\n",
    "    #Best run on test set MSE models=8, gates=4: 0.017976\n",
    "    \n",
    "    num_models=8\n",
    "    models=[nn(i, H) for i in range(num_models)]\n",
    "    num_gates=4\n",
    "    \n",
    "    moe=moe_(expert_nodes, num_models)\n",
    "    sparse_layer=sparseGating(moe.output, gates=num_gates)\n",
    "    layer=Activation(\"softmax\",name='act')(sparse_layer)\n",
    "\n",
    "    layer=gating_multiplier(layer,[m.layers[-1].output for m in models])\n",
    "    model=Model(inputs=nn_inputs, outputs=layer)\n",
    "\n",
    "    checkpointer=ModelCheckpoint('sparse.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "    model.compile(loss=moeLoss, optimizer=SGD(lr=0.1))\n",
    "elif(model_to_run=='moe_pretrained_elu'):\n",
    "    #Best run on test set MSE: 0.0010685\n",
    "    num_models=4\n",
    "    models=[nn(i, H) for i in range(num_models)]\n",
    "\n",
    "    #preTrain()\n",
    "    moe=moe_(expert_nodes, num_models)\n",
    "    layer=Activation(\"elu\",name='act')(moe.output)\n",
    "    layer=gating_multiplier(layer,[m.layers[-1].output for m in models])\n",
    "    model=Model(inputs=nn_inputs, outputs=layer)\n",
    "    load_weights(model)\n",
    "    checkpointer=ModelCheckpoint('moe_pretrained_elu.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "    model.compile(loss='mse', optimizer=SGD(lr=0.1))\n",
    "\n",
    "elif(model_to_run=='moe_pretrained'):\n",
    "    #Best run on test set MSE: 0.01444\n",
    "    num_models=4\n",
    "    models=[nn(i, H) for i in range(num_models)]\n",
    "\n",
    "    preTrain('base1')\n",
    "    moe=moe_(expert_nodes, num_models)\n",
    "    layer=gating_multiplier(moe.output,[m.layers[-1].output for m in models])\n",
    "    model=Model(inputs=nn_inputs, outputs=layer)\n",
    "    load_weights(model,'base1')\n",
    "    checkpointer=ModelCheckpoint('moe_pretrained.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "    model.compile(loss='mse', optimizer=SGD(lr=0.1))\n",
    "    \n",
    "elif(model_to_run=='sparse_pretrained'):\n",
    "    #Best run on test set MSE models=8, gates=2: 0.00907\n",
    "    #Best run on test set MSE models=8, gates=4: 0.00049734\n",
    "    \n",
    "    num_models=8\n",
    "    models=[nn(i, H) for i in range(num_models)]\n",
    "    preTrain('base1')\n",
    "    moe=moe_(expert_nodes, num_models)\n",
    "    num_gates=4\n",
    "    \n",
    "    layer=sparseGating(moe.output, gates=num_gates)\n",
    "    layer=Activation(\"softmax\",name='act')(layer)\n",
    "    layer=gating_multiplier(layer,[m.layers[-1].output for m in models])\n",
    "    model=Model(inputs=nn_inputs, outputs=layer)\n",
    "    \n",
    "    load_weights(model,'base1')\n",
    "    checkpointer=ModelCheckpoint('sparse_pretrained.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "\n",
    "    model.load_weights('sparse_pretrained.h5')\n",
    "    model.compile(loss='mse', optimizer=SGD(lr=0.1))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "elif(model_to_run=='sparse_pretrained_elu'):\n",
    "    #Best run on test set MSE models=8, gates=2: 0.004674\n",
    "    #Best run on test set MSE models=8, gates=4: 0.001323\n",
    "    \n",
    "    num_models=8\n",
    "    models=[nn(i, H) for i in range(num_models)]\n",
    "    preTrain('base1')\n",
    "    moe=moe_(expert_nodes, num_models)\n",
    "    num_gates=4\n",
    "\n",
    "    layer=Activation(\"elu\",name='act2')(moe.output)\n",
    "    \n",
    "    layer=sparseGating(layer, gates=num_gates)\n",
    "    layer=Activation(\"softmax\",name='act')(layer)\n",
    "    layer=gating_multiplier(layer,[m.layers[-1].output for m in models])\n",
    "    model=Model(inputs=nn_inputs, outputs=layer)\n",
    "    \n",
    "    load_weights(model,'base1')\n",
    "    checkpointer=ModelCheckpoint('sparse_pretrained_elu.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "    model.compile(loss='mse', optimizer=SGD(lr=0.1))\n",
    "\n",
    "\n",
    "model.fit(train_x,train_y,epochs=100, verbose=1,batch_size=1,callbacks=[checkpointer])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
